<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>open data | small DATA</title><link>https://smalldata.dev/tag/open-data/</link><atom:link href="https://smalldata.dev/tag/open-data/index.xml" rel="self" type="application/rss+xml"/><description>open data</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 22 Apr 2020 00:00:00 +0000</lastBuildDate><image><url>https://smalldata.dev/images/icon_hu137b7eee8aa8b920cff43510025d0585_16891_512x512_fill_lanczos_center_2.png</url><title>open data</title><link>https://smalldata.dev/tag/open-data/</link></image><item><title>Data Science and PAW Patrol</title><link>https://smalldata.dev/posts/paw-patrol/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://smalldata.dev/posts/paw-patrol/</guid><description>&lt;h2 id="step-0-the-disclaimers">Step 0. The disclaimers&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>I don&amp;rsquo;t call myself a &lt;em>data scientist&lt;/em>, but other people do, and I wanted to share my approach to &lt;em>data science&lt;/em>. This approach has worked well for me professionally. There are many good approaches, and I don&amp;rsquo;t want to gatekeep any of those.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PAW Patrol is a registered trademark of someone else. I don&amp;rsquo;t own any rights to PAW Patrol. If I did, I would be doing something better with my free time.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="step-1-start-with-the-science">Step 1. Start with the science&lt;/h2>
&lt;p>Thinking like a scientist means having a question, and then searching out an answer. Science isn&amp;rsquo;t data-driven, in that the data can provide an answer, but the available data shouldn&amp;rsquo;t suggest the question. At least in most of the cases I can think of.&lt;/p>
&lt;p>This is a huge challenge to productionalize data science in a business environment. With a recent (unnamed) employer, we had a big data-engineering effort to rebuild an analysis pipeline. As an analyst, it became slowly clear that with the new pipeline, we could only recreate 5 of the 6 key metrics. I left the company before the proposed workaround was implemented.&lt;/p>
&lt;p>For my example of methodology, I&amp;rsquo;ll use
&lt;a href="https://en.wikipedia.org/wiki/PAW_Patrol" target="_blank" rel="noopener">PAW Patrol&lt;/a>. It&amp;rsquo;s a kids&amp;rsquo; animated TV series. To describe cast of characters, and the &lt;em>plot&lt;/em>, here are the lyrics to the theme song:&lt;/p>
&lt;blockquote>
&lt;p>&lt;sub>PAW Patrol, PAW Patrol&lt;/sub>&lt;/p>
&lt;p>&lt;sub>We&amp;rsquo;ll be there on the double&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Whenever there&amp;rsquo;s a problem&lt;/sub>&lt;/p>
&lt;p>&lt;sub>'Round Adventure Bay&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Ryder and his team of pups&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Will come and save the day&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Marshall, Rubble, Chase&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Rocky, Zuma, Skye&lt;/sub>&lt;/p>
&lt;p>&lt;sub>Yeah! They&amp;rsquo;re on the way!&lt;/sub>&lt;/p>
&lt;/blockquote>
&lt;p>Yeah. It goes on. You get the point. But the show can be endearing, and I think the deeper reason it resonates with kids is because the team is useful and appreciated by the adults they help. But that&amp;rsquo;s a topic for another article.&lt;sup>
[1]&lt;/sup>&lt;/p>
&lt;p>If you take a vote in my household, a consensus of 50% of voters will declare PAW Patrol to be the single best achievement of humanity. And it&amp;rsquo;s been decided, by compromise, that if we ever get a pet dog, the name shall be either &lt;code>Zuma-Marshall&lt;/code>, or &lt;code>Marshall-Zuma&lt;/code>.&lt;/p>
&lt;p>&lt;img src="https://smalldata.dev/images/paw-patrol-jump-i49016.jpg" alt="paw patrol team">&lt;/p>
&lt;h3 id="the-question">The question&lt;/h3>
&lt;p>I was curious - with PAW Patrol hype slowly sweeping the planet since 2013, do we see an increase in dogs named after the crew?&lt;/p>
&lt;h2 id="step-2-curate-the-data">Step 2. Curate the data&lt;/h2>
&lt;p>I need open data with pet names. Dog names. There are several, and I&amp;rsquo;ll outline why they make a good or bad candidate for the analysis.&lt;/p>
&lt;h3 id="hundenamen-aus-dem-hundebestand-der-stadt-zÃ¼richhttpsdatastadt-zuerichchdatasetsid_stapo_hundenamen">
&lt;a href="https://data.stadt-zuerich.ch/dataset/sid_stapo_hundenamen" target="_blank" rel="noopener">Hundenamen aus dem Hundebestand der Stadt ZÃ¼rich&lt;/a>&lt;/h3>
&lt;p>This one is from the city of ZÃ¼rich, Switzerland, where I live. I&amp;rsquo;ve seen a
&lt;a href="https://twitter.com/OpenDataZurich/status/1235887129047240704" target="_blank" rel="noopener">recent Twitter post&lt;/a> about this dataset, so that may have planted the idea that dog names can be open data.&lt;/p>
&lt;p>Data goes back to 2015, and each year is one CSV file. To get an idea of the dataset size, I choose the complete year of
&lt;a href="https://data.stadt-zuerich.ch/dataset/sid_stapo_hundenamen/resource/37af5c6e-c101-41aa-8072-033280191f9c" target="_blank" rel="noopener">2019&lt;/a>. 7647 records. It may be hard to find trends in so few dog registrations. Additionally, the Paw Patrol trend is slowly making it here to Switzerland. Since it started in North America, I&amp;rsquo;ll go to look there.&lt;/p>
&lt;h3 id="anchorage-dog-names-over-timehttpscatalogdatagovdatasetdog-names-over-time">
&lt;a href="https://catalog.data.gov/dataset/dog-names-over-time" target="_blank" rel="noopener">Anchorage Dog Names over Time&lt;/a>&lt;/h3>
&lt;p>Only 16k total names between 2017 and 2019. That&amp;rsquo;s not enough dogs when there are so many possible names. And starting in 2017, I may not get a good &lt;em>before&lt;/em> snapshot.&lt;/p>
&lt;h3 id="seattle-pet-licenseshttpscatalogdatagovdatasetseattle-pet-licenses">
&lt;a href="https://catalog.data.gov/dataset/seattle-pet-licenses" target="_blank" rel="noopener">Seattle Pet Licenses&lt;/a>&lt;/h3>
&lt;blockquote>
&lt;p>A list of active/current Seattle pet licenses, including animal type (species), pet&amp;rsquo;s name, breed and the owner&amp;rsquo;s ZIP code.&lt;/p>
&lt;/blockquote>
&lt;p>This might be a good dataset because records go back to 2000 and are updated through 2019. I can get snapshots before and during the PAW Patrol era. But I counted dogs registered in 2019 and it was 11k. In 2018, 7k. Still not enough.&lt;/p>
&lt;h3 id="nyc-dog-licensing-datasethttpsdatacityofnewyorkushealthnyc-dog-licensing-datasetnu7n-tubp">
&lt;a href="https://data.cityofnewyork.us/Health/NYC-Dog-Licensing-Dataset/nu7n-tubp" target="_blank" rel="noopener">NYC Dog Licensing Dataset&lt;/a>&lt;/h3>
&lt;p>This could be it. Recently updated. 24.1 MB CSV file. 345k total rows going back more than 10 years. 79k dog registrations in 2019. Explore the data
&lt;a href="https://data.cityofnewyork.us/Health/NYC-Dog-Licensing-Dataset/nu7n-tubp/data" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>The fine print:&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Each record stands as a unique license period for the dog over the course of the yearlong time frame.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>What does this mean for my data? It means that dog names are assigned at least once per year. If I count unique dog names over multiple years, I&amp;rsquo;ll be over counting.&lt;/p>
&lt;p>and&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Each record represents a unique dog license that was active during the year, but not necessarily a unique record per dog, since a license that is renewed during the year results in a separate record of an active license period.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>This means that dog-names within a given year may actually be duplicate as well. If this was a real project, in order to fully trust my data, I would first count how many names are repeated. To do this, because there is no column &lt;code>dog ID&lt;/code> which would uniquely identify a dog, I would have to create a surrogate key based on the columns such as &lt;code>AnimalBirthMonth&lt;/code>, &lt;code>AnimalGender&lt;/code> and &lt;code>BreedName&lt;/code>, and perhaps also the geographical data &lt;code>Borough&lt;/code> and &lt;code>ZipCode&lt;/code>.&lt;/p>
&lt;p>If I cared more about PAW Patrol pet-naming theory, I could build a unified dog-name data-model and combine datasets, and keep looking for new sources to add. Maybe even make some FOIA requests. But I don&amp;rsquo;t care that much. On to the answer. Let&amp;rsquo;s hope NYC is a trend-setter &amp;ndash; when it comes to naming pets after kids&amp;rsquo; cartoon superheros.&lt;/p>
&lt;h2 id="step-3-explore-the-data">Step 3. Explore the data&lt;/h2>
&lt;p>Lots of tools but because I &lt;em>know&lt;/em> that I won&amp;rsquo;t productionalize this pipeline, I&amp;rsquo;m going to dump the data into the commercial BI tool Tableau Desktop. Licenses are expensive, and there are many solid open source tools that will do the job. But the software allows me to do fast visual anaylsis. If you sign up for the
&lt;a href="https://www.coursera.org/learn/analytics-tableau" target="_blank" rel="noopener">free audit of this Coursera&lt;/a>, you&amp;rsquo;ll see that after clicking through the 3rd week or so, you are provided a 6-month software license to complete the rest of the course. &lt;em>YMMV.&lt;/em>&lt;/p>
&lt;h3 id="visual-analysis">Visual Analysis&lt;/h3>
&lt;p>In this step, I&amp;rsquo;m trying to get a feel for the data. Is it clean, is it reliable? Will it answer my question?&lt;/p>
&lt;p>What are the 5 most common names for dogs in NYC?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">&lt;sub>Name&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>Frequency&lt;/sub>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">&lt;sub>UNKNOWN&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>5379&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>BELLA&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>3824&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>NAME NOT PROVIDED&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>3763&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>MAX&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>3582&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>CHARLIE&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2852&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>COCO&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2636&lt;/sub>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Woof. That&amp;rsquo;s a lot of &lt;em>NULL&lt;/em>. Let&amp;rsquo;s see how it&amp;rsquo;s evenly distributed throughout the years&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">&lt;sub>AnimalName&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2014&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2015&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2016&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2017&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2018&lt;/sub>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">&lt;sub>UNKNOWN&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>3&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1179&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1903&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1294&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1000&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>BELLA&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>19&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>427&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1332&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1233&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>813&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>NAME NOT PROVIDED&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>11&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>963&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1374&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>847&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>568&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>MAX&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>34&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>444&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1214&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1166&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>724&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>CHARLIE&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>26&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>293&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1027&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>935&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>571&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>COCO&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>22&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>296&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>941&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>817&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>560&lt;/sub>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>OK, better. It seems pretty flat, except for 2014. Let&amp;rsquo;s do a quick check of total names per year, to see that indeed 2014 has many fewer.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">&lt;sub>Year of LicenseIssuedDate&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>Frequency&lt;/sub>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">&lt;sub>2014&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2650&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>2015&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>42439&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>2016&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>119080&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>2017&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>110995&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>2018&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>70563&lt;/sub>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Bummer that 2019 registrations aren&amp;rsquo;t there yet. I hope NYC is trend-setting enough to show a signal in 2018.&lt;/p>
&lt;p>Another learning is that we should exclude 2014 from the calculation &amp;ndash; there are just not enough records, especially in comparison with other years.&lt;/p>
&lt;h3 id="initial-calculation">Initial Calculation&lt;/h3>
&lt;p>I want to start to measure trends, and I&amp;rsquo;ll do so visually, since I need to only compare a handful of names.&lt;/p>
&lt;p>Marshall, Rubble, Chase, Rocky, Zuma and Skye. Let&amp;rsquo;s add Ryder, even though he is a human in the cartoon. And the pups added in later seasons: Everest and Tracker.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="left">&lt;sub>AnimalName&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2014&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2015&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2016&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2017&lt;/sub>&lt;/th>
&lt;th align="center">&lt;sub>2018&lt;/sub>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="left">&lt;sub>CHASE&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>7&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>49&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>115&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>170&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>86&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>EVEREST&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>9&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>5&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>3&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>MARSHALL&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>7&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>26&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>19&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>11&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>ROCKY&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>20&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>300&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>823&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>785&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>486&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>RUBBLE&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>6&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>RYDER&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>13&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>28&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>26&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>18&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>SKYE&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>10&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>61&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>54&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>36&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>TRACKER&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>1&lt;/sub>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="left">&lt;sub>ZUMA&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2&lt;/sub>&lt;/td>
&lt;td align="center">&lt;sub>2&lt;/sub>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A couple things to notice. There aren&amp;rsquo;t enough Trackers, Rubbles or Zumas to use in any analysis. And Everest probably doesn&amp;rsquo;t have enough data to compare, either, especially since she was a late addition to the pack.&lt;/p>
&lt;p>And because the yearly totals of registrations varies so greatly, I need to normalize my data. This means that I take the count of each unique name and divide by the total dogs registered in that year. Noticing that Rocky is the most popular from my list, and that in 2016 there were 823 Rockies registered, from a total of 119,080 registrations, I want to normalize for the prevalence of each name to a fixed integer, so that I can compare years. In effect, how popular a certain name was in that year. The metric is then be &lt;code>frequency of dog-name per 10000 dogs&lt;/code>.&lt;/p>
&lt;h3 id="closing-the-calculation">Closing the calculation&lt;/h3>
&lt;p>The math doesn&amp;rsquo;t get too complicated, because the data doesn&amp;rsquo;t require anything else. I&amp;rsquo;m going to use my normalized frequency metric to compare trends for Chase, Marshall, Rocky, Ryder, Rocky and Skye.&lt;/p>
&lt;p>I&amp;rsquo;ve removed 2016 and 2017 now too, to try to more clearly show before and after. It&amp;rsquo;s only Skye that shows any trend &amp;ndash; effectively doubling in popularity.&lt;/p>
&lt;p>&lt;img src="https://smalldata.dev/images/paw_patrol_results.png" alt="paw patrol results">&lt;/p>
&lt;h2 id="step-4-shutting-it-down">Step 4. Shutting it down&lt;/h2>
&lt;p>I can&amp;rsquo;t believe I&amp;rsquo;ve spent this much time thinking about PAW Patrol. What I want to stress is that when you are working with data, for me it has been essential to&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Understand the subject matter. In this case, that means passively absorbing all things PAW Patrol.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Get dirty with the data. If you try to plug an algorithm on top of data, it&amp;rsquo;s unlikely to give any meaningful results. Can you imagine creating a fancy data pipeline that spits out the 3 most popular dog names? &lt;code>UNKNOWN&lt;/code>, &lt;code>BELLA&lt;/code> and &lt;code>NAME NOT PROVIDED&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In case you haven&amp;rsquo;t heard it already, here&amp;rsquo;s their theme song. I&amp;rsquo;ve been singing it to myself as I write this, and I hope it gets stuck in your head, too.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/1UdI_eoDPKQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h1 id="heading">ðŸ¶ðŸ¦´&lt;/h1>
&lt;h3 id="and-remember--_whenever-theres-trouble-just-yelp-for-help_">And remember&amp;ndash;&lt;em>Whenever thereâ€™s trouble, just yelp for help!&lt;/em>&lt;/h3>
&lt;hr>
&lt;p>&lt;sub>Got a comment? Share your thoughts on
&lt;a href="https://twitter.com/philshem/status/1252861430723837956" target="_blank" rel="noopener">Twitter&lt;/a> or
&lt;a href="https://news.ycombinator.com/item?id=22943114" target="_blank" rel="noopener">HackerNews&lt;/a>.&lt;/sub>&lt;/p>
&lt;hr>
&lt;h3 id="footnote">Footnote&lt;/h3>
&lt;p>&lt;sub>[1] For a less fun look at PAW Patrol&amp;rsquo;s effect on society, take a look at this recent academic paper:&lt;/sub>&lt;/p>
&lt;p>&lt;sub>
&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/1741659020903700" target="_blank" rel="noopener">&lt;em>&amp;ldquo;Whenever thereâ€™s trouble, just yelp for help&amp;rdquo;: Crime, conservation, and corporatization in Paw Patrol&lt;/em>&lt;/a>&lt;/sub>&lt;/p>
&lt;blockquote>
&lt;p>&lt;sub>I argue that the series suggests to audiences that we can and should rely on corporations and technological advancements to combat crime and conserve, with responsibilized individuals assisting in this endeavor.&lt;/sub>&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&lt;sub>Ultimately, PAW Patrol echoes core tenets of neoliberalism and encourages complicity in a global capitalist system that (re)produces inequalities and causes environmental harms.&lt;/sub>&lt;/p>
&lt;/blockquote></description></item><item><title>Open Access Research</title><link>https://smalldata.dev/posts/open-access-research/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><guid>https://smalldata.dev/posts/open-access-research/</guid><description>&lt;p>&lt;em>In my definition of Open Anything, &amp;ldquo;open&amp;rdquo; is an imperative verb.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Unless you are an academic publisher, you&amp;rsquo;re probably not a big fan of for-profit academic publishing.&lt;/p>
&lt;p>The business model of many journals is to charge the authors publication fees, and then charge the readers access fees. Double dipping - it&amp;rsquo;s what the credit card companies do.&lt;/p>
&lt;p>What&amp;rsquo;s happening as a response?
&lt;a href="https://www.nature.com/articles/d41586-019-00758-x" target="_blank" rel="noopener">Universities are dropping subscriptions&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>The move is the latest in an escalating global row between scholarly publishers and academic institutions, which are pushing to make more of the scientific literature freely available and say that the costs of publishersâ€™ subscriptions are becoming unreasonably expensive.&lt;/p>
&lt;/blockquote>
&lt;p>
&lt;a href="https://news.ycombinator.com/item?id=17688336" target="_blank" rel="noopener">And non-legal portals like sci-hub.tw are springing up&lt;/a>.&lt;/p>
&lt;p>Open access to academic research is not only important for top universities like UC Berkley, it&amp;rsquo;s essential for researchers around the world. A few examples that come to mind:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>amateurs and hobbyists&lt;/p>
&lt;/li>
&lt;li>
&lt;p>citizen science and community groups&lt;/p>
&lt;/li>
&lt;li>
&lt;p>high school students or others without university library access&lt;/p>
&lt;/li>
&lt;li>
&lt;p>researchers in developing economies&lt;/p>
&lt;/li>
&lt;li>
&lt;p>those doing machine learning projects on bulk academic papers&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="creating-a-self-archive">Creating a self-archive&lt;/h2>
&lt;p>I&amp;rsquo;ve read the articles about Elsevier and Springer but since I&amp;rsquo;m not publishing anymore, I hadn&amp;rsquo;t made much connection to my role in open access research.&lt;/p>
&lt;p>But for a freelance data-project I&amp;rsquo;m working on with an NGO, I have to match citation metadata and full text PDF for more accurate screening, both manual and with machine learning. Getting full-text PDFs is a tiresome process, and we are looking for 1000s of papers.&lt;/p>
&lt;p>Despite that my academic publishing ended 7 years ago, I wasn&amp;rsquo;t sure if my old research papers were openly available. Some of [my papers](
&lt;a href="https://scholar.google.ch/citations?user=SczBp6kAAAAJ&amp;amp;hl=en&amp;amp;authuser=1&amp;amp;oi=ao" target="_blank" rel="noopener">google scholar&lt;/a>
) are still being cited! (Actually, I had to dig deep to find my PDF copies!)&lt;/p>
&lt;p>It turns out that I still do have PDF copies, and that many of my old papers are not easily available to the public.&lt;/p>
&lt;p>Authors are
&lt;a href="https://tomsaunders.co.nz/how-to-make-your-research-open-access/" target="_blank" rel="noopener">allowed to share copies of post-prints&lt;/a>, and so that is what I&amp;rsquo;m finally doing.&lt;/p>
&lt;hr>
&lt;h2 id="handy-tools-for-creating-a-self-archive">Handy tools for creating a self-archive&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>
&lt;a href="https://www.bioinformatics.org/texmed/" target="_blank" rel="noopener">Export Pubmed citations as BibTex format&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;a href="https://flamingtempura.github.io/bibtex-tidy/" target="_blank" rel="noopener">Tidy up and standardize BibTex format&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;a href="https://blog.archive.org/2017/01/25/see-something-save-something/" target="_blank" rel="noopener">Archive your archive on the wayback machine&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h1 id="stop-reading-the-remaining-text-is-for-search-engine-indexing">&lt;em>Stop reading! The remaining text is for search engine indexing.&lt;/em>&lt;/h1>
&lt;p>&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>
&lt;br/>&lt;br/>&lt;/p>
&lt;hr>
&lt;p>##Bibliography&lt;/p>
&lt;p>###Research Papers&lt;/p>
&lt;p>2012&lt;/p>
&lt;p>P.T. Shemella, T. Laino, O. Fritz, A. Curioni&lt;/p>
&lt;p>Understanding the self-healing hydrophobic recovery of high-voltage insulators&lt;/p>
&lt;p>Journal of Physical Chemistry B 116 (24), 7351â€“7356&lt;/p>
&lt;p>
&lt;a href="https://pubs.acs.org/doi/10.1021/jp300786s" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/jp300786s.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091453/https://smalldata.dev/posts/open-access-research/jp300786s.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid22624664,
title = {{{U}nderstanding the self-healing hydrophobic recovery of high-voltage insulators}},
author = {Shemella, P. T. and Laino, T. and Fritz, O. and Curioni, A.},
year = 2012,
month = jun,
journal = {J Phys Chem B},
volume = 116,
number = 24,
pages = {7351--7356}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2011&lt;/p>
&lt;p>P.T. Shemella, T. Laino, O. Fritz, A. Curioni&lt;/p>
&lt;p>Surface dynamics of amorphous polymers used for high-voltage insulators&lt;/p>
&lt;p>Journal of Physical Chemistry B 115 (46), 13508â€“13512&lt;/p>
&lt;p>
&lt;a href="https://pubs.acs.org/doi/abs/10.1021/jp207589p" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/jp207589p.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091545/https://smalldata.dev/posts/open-access-research/jp207589p.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid22026429,
title = {{{S}urface dynamics of amorphous polymers used for high-voltage insulators}},
author = {Shemella, P. T. and Laino, T. and Fritz, O. and Curioni, A.},
year = 2011,
month = nov,
journal = {J Phys Chem B},
volume = 115,
number = 46,
pages = {13508--13512}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2011&lt;/p>
&lt;p>P.T. Shemella, T. Laino, O. Fritz, A. Curioni&lt;/p>
&lt;p>Molecular motion of amorphous silicone polymers&lt;/p>
&lt;p>Journal of Physical Chemistry B 115 (12) 2831â€“2835&lt;/p>
&lt;p>
&lt;a href="https://pubs.acs.org/doi/abs/10.1021/jp111318d" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/jp111318d.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091621/https://smalldata.dev/posts/open-access-research/jp111318d.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid21370843,
title = {{{M}olecular motion of amorphous silicone polymers}},
author = {Shemella, P. T. and Laino, T. and Fritz, O. and Curioni, A.},
year = 2011,
month = mar,
journal = {J Phys Chem B},
volume = 115,
number = 12,
pages = {2831--2835}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2011&lt;/p>
&lt;p>P.T. Shemella, N. Topilina, I. Soga, B. Pereira, G. Belfort, M. Belfort, S.K. Nayak&lt;/p>
&lt;p>Electronic structure of neighboring extein residue modulates intein C-terminal cleavage activity&lt;/p>
&lt;p>Biophysical Journal 100 (9), 2217â€“2225&lt;/p>
&lt;p>
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3149237/" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2011_Shemella_Biophys_J.pdf">pdf&lt;/a> (
&lt;a href="https://smalldata.dev/pdfs/2011_Shemella_Biophys_J_Supp.pdf">supplement&lt;/a>) -
&lt;a href="https://web.archive.org/web/20190521091804/https://smalldata.dev/posts/open-access-research/2011_Shemella_Biophys_J.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid21539790,
title = {{{E}lectronic structure of neighboring extein residue modulates intein {C}-terminal cleavage activity}},
author = {Shemella, P. T. and Topilina, N. I. and Soga, I. and Pereira, B. and Belfort, G. and Belfort, M. and Nayak, S. K.},
year = 2011,
month = may,
journal = {Biophys. J.},
volume = 100,
number = 9,
pages = {2217--2225}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2011&lt;/p>
&lt;p>B. Pereira*, P.T. Shemella*, G. Amitai, G. Belfort, S.K. Nayak, M. Belfort&lt;/p>
&lt;p>Spontaneous proton transfer to a conserved intein residue determines on-pathway protein splicing&lt;/p>
&lt;p>Journal of Molecular Biology 406 (3), 430â€“442 (*Joint first-authorship)&lt;/p>
&lt;p>
&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3034788/" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2011_Pereira_JMB.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091832/https://smalldata.dev/posts/open-access-research/2011_Pereira_JMB.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid21185311,
title = {{{S}pontaneous proton transfer to a conserved intein residue determines on-pathway protein splicing}},
author = {Pereira, B. and Shemella, P. T. and Amitai, G. and Belfort, G. and Nayak, S. K. and Belfort, M.},
year = 2011,
month = feb,
journal = {J. Mol. Biol.},
volume = 406,
number = 3,
pages = {430--442}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2010&lt;/p>
&lt;p>J.B. Thomas, E.B. Watson, F.S. Spear, P.T. Shemella, S.K. Nayak, A. Lanzirotti&lt;/p>
&lt;p>TitaniQ under pressure: the effect of pressure and temperature on the solubility of Ti in quartz&lt;/p>
&lt;p>Contributions to Mineralogy and Petrology 160 (5), 743â€“759&lt;/p>
&lt;p>
&lt;a href="https://doi.org/10.1007/s00410-010-0505-3" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2010_Thomas_CMP.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091901/https://smalldata.dev/posts/open-access-research/2010_Thomas_CMP.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{thomas2010titaniq,
title = {TitaniQ under pressure: the effect of pressure and temperature on the solubility of Ti in quartz},
author = {Thomas, Jay B and Watson, E Bruce and Spear, Frank S and Shemella, Philip T and Nayak, Saroj K and Lanzirotti, Antonio},
year = 2010,
journal = {Contributions to Mineralogy and Petrology},
publisher = {Springer},
volume = 160,
number = 5,
pages = {743--759}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2009&lt;/p>
&lt;p>Z. Du, P.T. Shemella, Y. Liu, S. McCallum, B. Pereira, S.K. Nayak, G. Belfort, M. Belfort, C. Wang&lt;/p>
&lt;p>Highly conserved histidine plays a dual catalytic role in protein splicing: A pKa shift mechanism&lt;/p>
&lt;p>Journal of the American Chemical Society 131 (32), 11581â€“11589&lt;/p>
&lt;p>
&lt;a href="https://pubs.acs.org/doi/abs/10.1021/ja904318w" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2009_Du_JACS.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091923/https://smalldata.dev/posts/open-access-research/2009_Du_JACS.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid19630416,
title = {{{H}ighly conserved histidine plays a dual catalytic role in protein splicing: a p{K}a shift mechanism}},
author = {Du, Z. and Shemella, P. T. and Liu, Y. and McCallum, S. A. and Pereira, B. and Nayak, S. K. and Belfort, G. and Belfort, M. and Wang, C.},
year = 2009,
month = aug,
journal = {J. Am. Chem. Soc.},
volume = 131,
number = 32,
pages = {11581--11589}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2009&lt;/p>
&lt;p>P.T. Shemella, S.K. Nayak&lt;/p>
&lt;p>Electronic structure and band-gap modulation of graphene via substrate surface chemistry&lt;/p>
&lt;p>Applied Physics Letters 94, 032101 (cover article)&lt;/p>
&lt;p>
&lt;a href="https://aip.scitation.org/doi/abs/10.1063/1.3070238" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2009_Shemella_APL.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521091945/https://smalldata.dev/posts/open-access-research/2009_Shemella_APL.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{doi:10.1063/1.3070238,
title = {Electronic structure and band-gap modulation of graphene via substrate surface chemistry},
author = {Shemella,Philip and Nayak,Saroj K.},
year = 2009,
journal = {Applied Physics Letters},
volume = 94,
number = 3,
pages = 32101,
doi = {10.1063/1.3070238},
url = {https://doi.org/10.1063/1.3070238},
eprint = {https://doi.org/10.1063/1.3070238}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2007&lt;/p>
&lt;p>P.T. Shemella, Y. Zhang, M. Mailman, P.M. Ajayan, S.K. Nayak&lt;/p>
&lt;p>Energy gaps in zero-dimensional graphene nanoribbons&lt;/p>
&lt;p>Applied Physics Letters 91, 042101&lt;/p>
&lt;p>
&lt;a href="https://doi.org/10.1063/1.2761531" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2007_Shemella_APL.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521092014/https://smalldata.dev/posts/open-access-research/2007_Shemella_APL.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{doi:10.1063/1.2761531,
title = {Energy gaps in zero-dimensional graphene nanoribbons},
author = {Shemella,Philip and Zhang,Yiming and Mailman,Mitch and Ajayan,Pulickel M. and Nayak,Saroj K.},
year = 2007,
journal = {Applied Physics Letters},
volume = 91,
number = 4,
pages = 42101,
doi = {10.1063/1.2761531},
url = {https://doi.org/10.1063/1.2761531},
eprint = {https://doi.org/10.1063/1.2761531}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>2007&lt;/p>
&lt;p>P.T. Shemella, B. Pereira, Y. Zhang, P. van Roey, G. Belfort, S. Garde, S.K. Nayak&lt;/p>
&lt;p>Mechanism for intein C-terminal cleavage: A proposal from quantum mechanical calculations&lt;/p>
&lt;p>Biophysical Journal 92 (3), 847â€“853 (2007)&lt;/p>
&lt;p>
&lt;a href="https://www.ncbi.nlm.nih.gov/entrez?Db=pubmed&amp;amp;Cmd=ShowDetailView&amp;amp;uid=17085503" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2007_Shemella_Biophysical_Journal.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521092041/https://smalldata.dev/posts/open-access-research/2007_Shemella_Biophysical_Journal.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@article{pmid17085503,
title = {{{M}echanism for intein {C}-terminal cleavage: a proposal from quantum mechanical calculations}},
author = {Shemella, P. and Pereira, B. and Zhang, Y. and Van Roey, P. and Belfort, G. and Garde, S. and Nayak, S. K.},
year = 2007,
month = feb,
journal = {Biophys. J.},
volume = 92,
number = 3,
pages = {847--853}
}
&lt;/code>&lt;/pre>
&lt;p>###Book Chapter&lt;/p>
&lt;p>2010&lt;/p>
&lt;p>P.T. Shemella, S.K. Nayak&lt;/p>
&lt;p>Identifying the reaction mechanisms of inteins with QM/MM multiscale methods&lt;/p>
&lt;p>Computational Modeling in Biomechanics, 469â€“489&lt;/p>
&lt;p>
&lt;a href="https://link.springer.com/chapter/10.1007/978-90-481-3575-2_16" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/Chapter1.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521092107/https://smalldata.dev/posts/open-access-research/Chapter1.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@inbook{computational2010,
title = {Identifying the Reaction Mechanisms of Inteins with QM/MM Multiscale Methods},
author = {Shemella, Philip T. and Nayak, Saroj},
year = 2010,
month = 11,
journal = {Computational Modeling in Biomechanics},
pages = {469--489},
doi = {10.1007/978-90-481-3575-2\_16}
}
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>###PhD Thesis&lt;/p>
&lt;p>2008&lt;/p>
&lt;p>First principles study of intein reaction mechanisms&lt;/p>
&lt;p>Department of Physics, Applied Physics &amp;amp; Astronomy, Rensselaer Polytechnic Institute&lt;/p>
&lt;p>
&lt;a href="https://arxiv.org/abs/0903.4570" target="_blank" rel="noopener">www&lt;/a> -
&lt;a href="https://smalldata.dev/pdfs/2008_Shemella_PhD_Thesis.pdf">pdf&lt;/a> -
&lt;a href="https://web.archive.org/web/20190521092517/https://smalldata.dev/posts/open-access-research/2008_Shemella_PhD_Thesis.pdf" target="_blank" rel="noopener">wayback archive&lt;/a>&lt;/p>
&lt;pre>&lt;code>@phdthesis{phdthesis,
title = {First Principles Study of Intein Reaction Mechanisms},
author = {Philip Shemella},
year = 2008,
month = 5,
school = {Rensselaer Polytechnic Institute},
eprint = {arXiv:0903.4570}
}
&lt;/code>&lt;/pre></description></item><item><title>Fun with Favicons</title><link>https://smalldata.dev/posts/favicon-mosaic/</link><pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate><guid>https://smalldata.dev/posts/favicon-mosaic/</guid><description>&lt;p>A recent
&lt;a href="https://opendata.stackexchange.com/q/14007/1511" target="_blank" rel="noopener">question&lt;/a> on the Open Data Stack Exchange site got me thinking about how to download favicons from a bulk list of websites.&lt;/p>
&lt;h3 id="idea-1-try-each-domain">Idea 1: try each domain&lt;/h3>
&lt;p>Something like &lt;code>http://example.com/favicon.ico&lt;/code>. But using a &lt;code>favicon.ico&lt;/code> in the webroot folder is just a common implementation. Each website can host their favicon with another path, and another file format.&lt;/p>
&lt;p>Let&amp;rsquo;s try something else&amp;hellip;&lt;/p>
&lt;h3 id="idea-2-parse-html-for-favicon-urls">Idea 2: parse html for favicon urls&lt;/h3>
&lt;p>If the website doesn&amp;rsquo;t use &lt;code>favicon.ico&lt;/code> in the webroot folder, the page html will contain a path to the favicon, with the following format:&lt;/p>
&lt;pre>&lt;code>&amp;lt;link rel=icon href=https://smalldata.dev/favicon.png&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>There is python package aptly named
&lt;a href="https://github.com/scottwernervt/favicon" target="_blank" rel="noopener">favicon&lt;/a> that will parse the html and return the urls to all favicons, with different formats and resolutions. I&amp;rsquo;m pasting their demo code here:&lt;/p>
&lt;pre>&lt;code class="language-python">&amp;gt;&amp;gt;&amp;gt; import favicon
&amp;gt;&amp;gt;&amp;gt; icons = favicon.get('https://www.python.org/')
Icon(url='https://www.python.org/static/apple-touch-icon-144x144-precomposed.png', width=144, height=144, format='png')
Icon(url='https://www.python.org/static/apple-touch-icon-114x114-precomposed.png', width=114, height=114, format='png')
Icon(url='https://www.python.org/static/apple-touch-icon-72x72-precomposed.png', width=72, height=72, format='png')
Icon(url='https://www.python.org/static/apple-touch-icon-precomposed.png', width=0, height=0, format='png')
Icon(url='https://www.python.org/static/favicon.ico', width=0, height=0, format='ico')
&lt;/code>&lt;/pre>
&lt;p>Getting better&amp;hellip; But if I download bulk favicons, I&amp;rsquo;d like to avoid normalizing their file format and resolutions.&lt;/p>
&lt;h3 id="idea-3-get-favicons-directly-from-googles-cache">Idea 3: get favicons directly from google&amp;rsquo;s cache&lt;/h3>
&lt;p>Google keeps the favicon cached for many sites (even my little website with basically zero traffic).&lt;/p>
&lt;pre>&lt;code>https://www.google.com/s2/favicons?domain=smalldata.dev
&lt;/code>&lt;/pre>
&lt;p>And the favicons are all normalized: 16x16 pixels and png format. Perfect.&lt;/p>
&lt;h2 id="now-for-some-fun">Now for some fun&lt;/h2>
&lt;p>A
&lt;a href="https://moz.com/top500" target="_blank" rel="noopener">top500 website list&lt;/a> has a
&lt;a href="https://moz.com/top500/domains/csv" target="_blank" rel="noopener">csv export&lt;/a> and wrote a Python script to download each of these 500 favicons from Google&amp;rsquo;s cache and save to local folder &lt;code>images/&lt;/code>.&lt;/p>
&lt;pre>&lt;code>import requests
import pandas as pd
import os
from io import StringIO
def request_function(domain):
domain = domain.replace('/','')
url = 'https://www.google.com/s2/favicons?domain=' + domain
fav = requests.get(url).content
with open('images'+os.sep+domain+'.png', 'wb') as handler:
handler.write(fav)
return
# top 500 websites from mozilla https://moz.com/top500
url = &amp;quot;https://moz.com:443/top500/domains/csv&amp;quot;
headers = {&amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0&amp;quot;}
req = requests.get(url, headers=headers)
data = StringIO(req.text)
df = pd.read_csv(data)
df.URL.apply(request_function)
&lt;/code>&lt;/pre>
&lt;h3 id="favicon-art">Favicon art&lt;/h3>
&lt;p>What to do with 500 favicons. For fun, I made a mosaic from the collection, and I first needed a original piece of art that would be recongnizable when heavily pixelated. Van Gogh&amp;rsquo;s
&lt;a href="https://en.wikipedia.org/wiki/The_Starry_Night" target="_blank" rel="noopener">Starry night&lt;/a> stood out.&lt;/p>
&lt;p>Here&amp;rsquo;s the original:&lt;/p>
&lt;p>&lt;img src="https://smalldata.dev/images/1137px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg" alt="Starry Night">&lt;/p>
&lt;p>&lt;em>Source&lt;/em>:
&lt;a href="https://en.wikipedia.org/wiki/The_Starry_Night#/media/File:Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg" target="_blank" rel="noopener">Wikipedia&lt;/a>&lt;/p>
&lt;p>Then I used a handy Python script called
&lt;a href="https://github.com/codebox/mosaic" target="_blank" rel="noopener">mosaic.py&lt;/a>. No coding necessary.&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/codebox/mosaic.git
python mosaic/mosaic.py source.jpg images/
&lt;/code>&lt;/pre>
&lt;p>And what pops out is a &lt;em>Starry Night of Favicons&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://smalldata.dev/images/mosaic.jpeg" alt="Favicon Starry Night">&lt;/p>
&lt;hr>
&lt;p>(full resolution download:
&lt;a href="https://smalldata.dev/images/mosaic_full.jpeg">22 MB&lt;/a>)&lt;/p>
&lt;p>(python
&lt;a href="https://gist.github.com/philshem/e59388197fd9ddb7dcdb8098f9f0aaf2" target="_blank" rel="noopener">source code&lt;/a>)&lt;/p>
&lt;p>(top500 favicons:
&lt;a href="https://smalldata.dev/images/top500_favicons.zip">zip&lt;/a>)&lt;/p></description></item></channel></rss>